{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance Claim Prediction - Enhanced Pipeline with Cross-Validation\n",
    "\n",
    "Complete ML pipeline with:\n",
    "- **Cross-Validation** instead of single train/validation split\n",
    "- Comprehensive EDA visualizations including feature-target relationships\n",
    "- **Per-model oversampling/undersampling analysis**\n",
    "- Intelligent application of SMOTE, ADASYN, undersampling, or hybrid methods\n",
    "- **Overfitting/Underfitting detection** for each configuration\n",
    "- Best model selection with extensive validation\n",
    "- Final test set evaluation with diagnostic visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, f1_score, \n",
    "                             roc_auc_score, roc_curve, auc, accuracy_score, \n",
    "                             precision_score, recall_score)\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"train_Insurance.csv\")\n",
    "test_raw = pd.read_csv(\"test_Insurance.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_raw.shape}\")\n",
    "print(f\"Test shape: {test_raw.shape}\")\n",
    "print(f\"\\nTrain columns: {list(train_raw.columns)}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(train_raw.head())\n",
    "print(f\"\\nData types:\")\n",
    "print(train_raw.dtypes)\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(train_raw.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = train_raw.isna().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    axes[0].bar(missing.index, missing.values, color='coral')\n",
    "    axes[0].set_title('Missing Values Count', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_xlabel('Columns')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    for i, v in enumerate(missing.values):\n",
    "        axes[0].text(i, v + 5, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    missing_pct = (missing / len(train_raw)) * 100\n",
    "    axes[1].barh(missing_pct.index, missing_pct.values, color='salmon')\n",
    "    axes[1].set_title('Missing Values Percentage', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Percentage (%)')\n",
    "    for i, v in enumerate(missing_pct.values):\n",
    "        axes[1].text(v + 0.2, i, f'{v:.1f}%', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "train_raw['Claim'].value_counts().plot.pie(\n",
    "    autopct='%1.1f%%',\n",
    "    ax=axes[0],\n",
    "    ylabel='',\n",
    "    colors=['#66b3ff', '#ff9999'],\n",
    "    title='Target Distribution',\n",
    "    startangle=90\n",
    ")\n",
    "\n",
    "claim_counts = train_raw['Claim'].value_counts()\n",
    "axes[1].bar(claim_counts.index, claim_counts.values, color=['#66b3ff', '#ff9999'])\n",
    "axes[1].set_title('Target Distribution', fontweight='bold')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xlabel('Claim')\n",
    "for i, v in enumerate(claim_counts.values):\n",
    "    axes[1].text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(train_raw['Claim'].value_counts())\n",
    "print(f\"\\nClass proportions:\")\n",
    "print(train_raw['Claim'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [col for col in train_raw.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "            if col not in [\"Claim\", \"Customer Id\"]]\n",
    "\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "\n",
    "n_cols = 2\n",
    "n_rows = math.ceil(len(cat_cols) / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 5 * n_rows))\n",
    "if n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(cat_cols):\n",
    "    sns.countplot(x=col, data=train_raw, ax=axes[i], palette='Set2')\n",
    "    axes[i].set_title(f\"Distribution of {col}\", fontweight='bold')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for container in axes[i].containers:\n",
    "        axes[i].bar_label(container)\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = train_raw.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "num_cols = [c for c in num_cols if c not in [\"Claim\", \"Customer Id\"]]\n",
    "\n",
    "print(f\"Numerical columns: {num_cols}\")\n",
    "\n",
    "n_cols = 2\n",
    "n_rows = math.ceil(len(num_cols) / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 5 * n_rows))\n",
    "if n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    sns.histplot(train_raw[col].dropna(), kde=True, ax=axes[i], color='steelblue', bins=30)\n",
    "    axes[i].set_title(f\"Distribution of {col}\", fontweight='bold')\n",
    "    axes[i].set_xlabel(col)\n",
    "    mean_val = train_raw[col].mean()\n",
    "    axes[i].axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.2f}')\n",
    "    axes[i].legend()\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove Customer ID\n",
    "train_raw.drop(labels=['Customer Id'], axis=1, inplace=True)\n",
    "test_raw.drop(labels=['Customer Id'], axis=1, inplace=True)\n",
    "\n",
    "# Step 2: Clean NumberOfWindows\n",
    "train_raw['NumberOfWindows'] = train_raw['NumberOfWindows'].replace({'without': 0, '>=10': 10})\n",
    "train_raw['NumberOfWindows'] = train_raw['NumberOfWindows'].astype(int)\n",
    "test_raw['NumberOfWindows'] = test_raw['NumberOfWindows'].replace({'without': 0, '>=10': 10})\n",
    "test_raw['NumberOfWindows'] = test_raw['NumberOfWindows'].astype(int)\n",
    "\n",
    "# Step 3: Remove Duplicates and Conflicts\n",
    "n_exact = train_raw.duplicated().sum()\n",
    "n_same_features = train_raw.duplicated(subset=[c for c in train_raw.columns if c != 'Claim']).sum()\n",
    "\n",
    "print(f\"Exact duplicates: {n_exact}\")\n",
    "print(f\"Duplicates with same features: {n_same_features}\")\n",
    "\n",
    "if n_same_features > 0:\n",
    "    features = [c for c in train_raw.columns if c != \"Claim\"]\n",
    "    dups = train_raw[train_raw.duplicated(subset=features, keep=False)]\n",
    "    conflicts = dups.groupby(features)['Claim'].nunique()\n",
    "    n_conflicts = (conflicts > 1).sum()\n",
    "    print(f\"Conflicting records: {n_conflicts}\")\n",
    "    \n",
    "    if n_conflicts > 0:\n",
    "        conflicting_groups = conflicts[conflicts > 1].reset_index()\n",
    "        before = len(train_raw)\n",
    "        train_raw = train_raw.merge(conflicting_groups[features], on=features, how='left', indicator=True)\n",
    "        train_raw = train_raw[train_raw['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "        after = len(train_raw)\n",
    "        print(f\"Removed {before - after} conflicting records\")\n",
    "\n",
    "train_raw.drop_duplicates(inplace=True)\n",
    "print(f\"Train shape after cleaning: {train_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Handle Missing Values\n",
    "print(\"Missing values in train:\")\n",
    "print(train_raw.isna().sum())\n",
    "\n",
    "mf_imputer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "train_raw[[\"Garden\"]] = mf_imputer.fit_transform(train_raw[[\"Garden\"]])\n",
    "\n",
    "median_imputer = SimpleImputer(strategy=\"median\")\n",
    "train_raw[[\"Building Dimension\"]] = median_imputer.fit_transform(train_raw[[\"Building Dimension\"]])\n",
    "\n",
    "# Step 5: Fill Missing Geo_Code\n",
    "mode_geo_train = (\n",
    "    train_raw[train_raw[\"Geo_Code\"].notna()]\n",
    "    .groupby([\"Settlement\", \"Residential\"])[\"Geo_Code\"]\n",
    "    .agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"Geo_Code\": \"Geo_Code_mode\"})\n",
    ")\n",
    "\n",
    "train_raw = train_raw.merge(mode_geo_train, on=[\"Settlement\", \"Residential\"], how=\"left\")\n",
    "train_raw[\"Geo_Code\"] = train_raw[\"Geo_Code\"].fillna(train_raw[\"Geo_Code_mode\"])\n",
    "train_raw = train_raw.drop(columns=[\"Geo_Code_mode\"])\n",
    "\n",
    "# Step 6: Clean Geo_Code (remove alphanumeric)\n",
    "mask_numeric = train_raw[\"Geo_Code\"].astype(str).str.isnumeric()\n",
    "train_raw = train_raw[mask_numeric].copy()\n",
    "train_raw[\"Geo_Code\"] = train_raw[\"Geo_Code\"].astype(int)\n",
    "\n",
    "print(f\"\\nTrain shape after Geo_Code cleaning: {train_raw.shape}\")\n",
    "print(f\"Missing values after imputation:\\n{train_raw.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Handle Outliers in Building Dimension\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.boxplot(x=train_raw['Building Dimension'], ax=axes[0], color='orange')\n",
    "axes[0].set_title('Building Dimension - Before Outlier Treatment', fontweight='bold')\n",
    "\n",
    "Q1 = train_raw['Building Dimension'].quantile(0.25)\n",
    "Q3 = train_raw['Building Dimension'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers_before = (train_raw['Building Dimension'] < lower) | (train_raw['Building Dimension'] > upper)\n",
    "print(f\"Outliers found: {outliers_before.sum()}\")\n",
    "\n",
    "train_raw['Building Dimension'] = train_raw['Building Dimension'].clip(lower, upper)\n",
    "\n",
    "sns.boxplot(x=train_raw['Building Dimension'], ax=axes[1], color='skyblue')\n",
    "axes[1].set_title('Building Dimension - After Outlier Treatment', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Scale Numerical Features\n",
    "cols_to_scale = ['Building Dimension', 'NumberOfWindows']\n",
    "scaler = RobustScaler()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for i, col in enumerate(cols_to_scale):\n",
    "    sns.histplot(train_raw[col], kde=True, ax=axes[i, 0], color='coral', bins=30)\n",
    "    axes[i, 0].set_title(f\"{col} - Before Scaling\", fontweight='bold')\n",
    "\n",
    "train_raw[cols_to_scale] = scaler.fit_transform(train_raw[cols_to_scale])\n",
    "\n",
    "for i, col in enumerate(cols_to_scale):\n",
    "    sns.histplot(train_raw[col], kde=True, ax=axes[i, 1], color='steelblue', bins=30)\n",
    "    axes[i, 1].set_title(f\"{col} - After RobustScaler\", fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì RobustScaler fitted and applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Encode Categorical Variables\n",
    "train_transformed = train_raw.copy()\n",
    "\n",
    "train_transformed[\"Building_Painted\"] = train_transformed[\"Building_Painted\"].map({'N': 1, 'V': 0}).astype('int32')\n",
    "train_transformed[\"Building_Fenced\"] = train_transformed[\"Building_Fenced\"].map({'N': 1, 'V': 0}).astype('int32')\n",
    "train_transformed[\"Garden\"] = train_transformed[\"Garden\"].map({'V': 1, 'O': 0}).astype('int32')\n",
    "\n",
    "train_transformed = pd.get_dummies(train_transformed, columns=[\"Settlement\", \"Building_Type\"], drop_first=True, dtype='int32')\n",
    "\n",
    "le_claim = LabelEncoder()\n",
    "train_transformed[\"Claim\"] = le_claim.fit_transform(train_transformed[\"Claim\"])\n",
    "\n",
    "cols = [c for c in train_transformed.columns if c != \"Claim\"] + [\"Claim\"]\n",
    "train_transformed = train_transformed[cols]\n",
    "\n",
    "print(\"‚úì Train data encoded\")\n",
    "print(f\"Train shape: {train_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-Target Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE-TARGET RELATIONSHIP ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Numerical features vs Target\n",
    "numerical_features = ['Building Dimension', 'NumberOfWindows', 'Geo_Code', \n",
    "                     'YearOfObservation', 'Insured_Period', 'Residential']\n",
    "\n",
    "existing_num_features = [f for f in numerical_features if f in train_transformed.columns]\n",
    "\n",
    "if len(existing_num_features) > 0:\n",
    "    n_cols = 2\n",
    "    n_rows = math.ceil(len(existing_num_features) / n_cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(existing_num_features):\n",
    "        for claim_val in [0, 1]:\n",
    "            data = train_transformed[train_transformed['Claim'] == claim_val][feature]\n",
    "            label = 'No Claim' if claim_val == 0 else 'Claim'\n",
    "            axes[i].hist(data, alpha=0.6, label=label, bins=30)\n",
    "        \n",
    "        axes[i].set_title(f'{feature} vs Claim', fontweight='bold')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(alpha=0.3)\n",
    "    \n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Categorical features vs Target\n",
    "categorical_encoded = ['Building_Painted', 'Building_Fenced', 'Garden']\n",
    "existing_cat_features = [f for f in categorical_encoded if f in train_transformed.columns]\n",
    "\n",
    "if len(existing_cat_features) > 0:\n",
    "    n_cols = 3\n",
    "    n_rows = math.ceil(len(existing_cat_features) / n_cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = np.array([axes]) if n_cols == 1 else axes\n",
    "    axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(existing_cat_features):\n",
    "        crosstab = pd.crosstab(train_transformed[feature], train_transformed['Claim'], normalize='index') * 100\n",
    "        crosstab.plot(kind='bar', ax=axes[i], color=['#66b3ff', '#ff9999'])\n",
    "        axes[i].set_title(f'{feature} vs Claim (%)', fontweight='bold')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Percentage')\n",
    "        axes[i].legend(['No Claim', 'Claim'])\n",
    "        axes[i].tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = train_transformed.corr(numeric_only=True)\n",
    "corr_with_claim = df_corr[[\"Claim\"]].sort_values(by=\"Claim\", ascending=False)\n",
    "\n",
    "print(\"Correlation with Claim:\")\n",
    "print(corr_with_claim)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "sns.heatmap(corr_with_claim, annot=True, vmin=-1, vmax=1, cmap='coolwarm', center=0, ax=axes[0])\n",
    "axes[0].set_title('Feature Correlation with Target', fontsize=14, fontweight='bold')\n",
    "\n",
    "top_features = corr_with_claim.abs().sort_values(by='Claim', ascending=False).head(10)\n",
    "top_features.plot(kind='barh', ax=axes[1], color='teal', legend=False)\n",
    "axes[1].set_title('Top 10 Features by Correlation Strength', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Absolute Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Drop low-correlation features\n",
    "cols_to_drop = [\n",
    "    'Building_Painted',\n",
    "    'Geo_Code',\n",
    "    'YearOfObservation',\n",
    "    'Building_Type_Non-combustible'\n",
    "]\n",
    "\n",
    "cols_to_drop = [c for c in cols_to_drop if c in train_transformed.columns]\n",
    "train_transformed = train_transformed.drop(columns=cols_to_drop)\n",
    "train_transformed = train_transformed.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n‚úì Final train shape: {train_transformed.shape}\")\n",
    "print(f\"‚úì Final features: {list(train_transformed.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Same Transformations to Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying transformations to test data...\")\n",
    "\n",
    "test_raw[[\"Garden\"]] = mf_imputer.transform(test_raw[[\"Garden\"]])\n",
    "test_raw[[\"Building Dimension\"]] = median_imputer.transform(test_raw[[\"Building Dimension\"]])\n",
    "\n",
    "test_raw = test_raw.merge(mode_geo_train, on=[\"Settlement\", \"Residential\"], how=\"left\")\n",
    "test_raw[\"Geo_Code\"] = test_raw[\"Geo_Code\"].fillna(test_raw[\"Geo_Code_mode\"])\n",
    "test_raw = test_raw.drop(columns=[\"Geo_Code_mode\"])\n",
    "\n",
    "mask_numeric_test = test_raw[\"Geo_Code\"].astype(str).str.isnumeric()\n",
    "test_raw = test_raw[mask_numeric_test].copy()\n",
    "test_raw[\"Geo_Code\"] = test_raw[\"Geo_Code\"].astype(int)\n",
    "\n",
    "test_raw[cols_to_scale] = scaler.transform(test_raw[cols_to_scale])\n",
    "\n",
    "test_transformed = test_raw.copy()\n",
    "test_transformed[\"Building_Painted\"] = test_transformed[\"Building_Painted\"].map({'N': 1, 'V': 0}).astype('int32')\n",
    "test_transformed[\"Building_Fenced\"] = test_transformed[\"Building_Fenced\"].map({'N': 1, 'V': 0}).astype('int32')\n",
    "test_transformed[\"Garden\"] = test_transformed[\"Garden\"].map({'V': 1, 'O': 0}).astype('int32')\n",
    "\n",
    "test_transformed = pd.get_dummies(test_transformed, columns=[\"Settlement\", \"Building_Type\"], drop_first=True, dtype='int32')\n",
    "test_transformed[\"Claim\"] = le_claim.transform(test_transformed[\"Claim\"])\n",
    "\n",
    "cols = [c for c in test_transformed.columns if c != \"Claim\"] + [\"Claim\"]\n",
    "test_transformed = test_transformed[cols]\n",
    "\n",
    "cols_to_drop_test = [c for c in cols_to_drop if c in test_transformed.columns]\n",
    "test_transformed = test_transformed.drop(columns=cols_to_drop_test)\n",
    "test_transformed = test_transformed.reset_index(drop=True)\n",
    "\n",
    "# Ensure columns match\n",
    "for col in train_transformed.columns:\n",
    "    if col not in test_transformed.columns:\n",
    "        test_transformed[col] = 0\n",
    "\n",
    "test_transformed = test_transformed[train_transformed.columns]\n",
    "\n",
    "print(f\"‚úì Final test shape: {test_transformed.shape}\")\n",
    "print(f\"‚úì Test columns match train: {list(train_transformed.columns) == list(test_transformed.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_transformed.drop('Claim', axis=1)\n",
    "y_train = train_transformed['Claim']\n",
    "\n",
    "X_test = test_transformed.drop('Claim', axis=1)\n",
    "y_test = test_transformed['Claim']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA PREPARED FOR CROSS-VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
    "print(f\"\\nTrain target distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nTest target distribution:\\n{y_test.value_counts()}\")\n",
    "\n",
    "minority_class = y_train.value_counts().min()\n",
    "majority_class = y_train.value_counts().max()\n",
    "imbalance_ratio = majority_class / minority_class\n",
    "\n",
    "print(f\"\\nClass Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"Minority class: {minority_class} samples\")\n",
    "print(f\"Majority class: {majority_class} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Models and Resampling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=8),\n",
    "    'SVM': SVC(probability=True, random_state=42, kernel='rbf')\n",
    "}\n",
    "\n",
    "resampling_strategies = {\n",
    "    'None (class_weight)': None,\n",
    "    'SMOTE': SMOTE(random_state=42),\n",
    "    'ADASYN': ADASYN(random_state=42),\n",
    "    'BorderlineSMOTE': BorderlineSMOTE(random_state=42),\n",
    "    'RandomUnderSampler': RandomUnderSampler(random_state=42),\n",
    "    'SMOTETomek': SMOTETomek(random_state=42)\n",
    "}\n",
    "\n",
    "print(\"‚úì Models defined:\")\n",
    "for name in base_models.keys():\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "print(\"\\n‚úì Resampling strategies:\")\n",
    "for name in resampling_strategies.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation with Multiple Resampling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CROSS-VALIDATION: TESTING ALL MODELS WITH ALL RESAMPLING STRATEGIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1': 'f1',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'roc_auc': 'roc_auc'\n",
    "}\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "for model_name, base_model in base_models.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for strategy_name, resampler in resampling_strategies.items():\n",
    "        print(f\"\\n  Strategy: {strategy_name}\", end=\"\")\n",
    "        \n",
    "        try:\n",
    "            if resampler is None:\n",
    "                # Use class_weight='balanced'\n",
    "                if hasattr(base_model, 'class_weight'):\n",
    "                    model = base_model.__class__(**{**base_model.get_params(), 'class_weight': 'balanced'})\n",
    "                else:\n",
    "                    model = base_model.__class__(**base_model.get_params())\n",
    "                \n",
    "                scores = cross_validate(model, X_train, y_train, cv=cv, \n",
    "                                       scoring=scoring, n_jobs=-1, return_train_score=True)\n",
    "            else:\n",
    "                # Use resampling strategy\n",
    "                model = base_model.__class__(**base_model.get_params())\n",
    "                \n",
    "                fold_scores = {'test_accuracy': [], 'test_f1': [], 'test_precision': [], \n",
    "                             'test_recall': [], 'test_roc_auc': [],\n",
    "                             'train_accuracy': [], 'train_f1': [], 'train_precision': [], \n",
    "                             'train_recall': [], 'train_roc_auc': []}\n",
    "                \n",
    "                for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "                    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "                    \n",
    "                    X_resampled, y_resampled = resampler.fit_resample(X_fold_train, y_fold_train)\n",
    "                    \n",
    "                    model.fit(X_resampled, y_resampled)\n",
    "                    \n",
    "                    # Validation scores\n",
    "                    y_val_pred = model.predict(X_fold_val)\n",
    "                    y_val_proba = model.predict_proba(X_fold_val)[:, 1]\n",
    "                    \n",
    "                    fold_scores['test_accuracy'].append(accuracy_score(y_fold_val, y_val_pred))\n",
    "                    fold_scores['test_f1'].append(f1_score(y_fold_val, y_val_pred))\n",
    "                    fold_scores['test_precision'].append(precision_score(y_fold_val, y_val_pred))\n",
    "                    fold_scores['test_recall'].append(recall_score(y_fold_val, y_val_pred))\n",
    "                    fold_scores['test_roc_auc'].append(roc_auc_score(y_fold_val, y_val_proba))\n",
    "                    \n",
    "                    # Training scores\n",
    "                    y_train_pred = model.predict(X_resampled)\n",
    "                    y_train_proba = model.predict_proba(X_resampled)[:, 1]\n",
    "                    \n",
    "                    fold_scores['train_accuracy'].append(accuracy_score(y_resampled, y_train_pred))\n",
    "                    fold_scores['train_f1'].append(f1_score(y_resampled, y_train_pred))\n",
    "                    fold_scores['train_precision'].append(precision_score(y_resampled, y_train_pred))\n",
    "                    fold_scores['train_recall'].append(recall_score(y_resampled, y_train_pred))\n",
    "                    fold_scores['train_roc_auc'].append(roc_auc_score(y_resampled, y_train_proba))\n",
    "                \n",
    "                scores = {k: np.array(v) for k, v in fold_scores.items()}\n",
    "            \n",
    "            result = {\n",
    "                'Model': model_name,\n",
    "                'Strategy': strategy_name,\n",
    "                'CV_Accuracy_Mean': scores['test_accuracy'].mean(),\n",
    "                'CV_Accuracy_Std': scores['test_accuracy'].std(),\n",
    "                'CV_F1_Mean': scores['test_f1'].mean(),\n",
    "                'CV_F1_Std': scores['test_f1'].std(),\n",
    "                'CV_Precision_Mean': scores['test_precision'].mean(),\n",
    "                'CV_Recall_Mean': scores['test_recall'].mean(),\n",
    "                'CV_ROC_AUC_Mean': scores['test_roc_auc'].mean(),\n",
    "                'Train_Accuracy_Mean': scores['train_accuracy'].mean(),\n",
    "                'Train_F1_Mean': scores['train_f1'].mean(),\n",
    "                'Train_ROC_AUC_Mean': scores['train_roc_auc'].mean(),\n",
    "            }\n",
    "            \n",
    "            # Detect overfitting/underfitting\n",
    "            overfit_gap_f1 = result['Train_F1_Mean'] - result['CV_F1_Mean']\n",
    "            overfit_gap_acc = result['Train_Accuracy_Mean'] - result['CV_Accuracy_Mean']\n",
    "            \n",
    "            result['Overfit_Gap_F1'] = overfit_gap_f1\n",
    "            result['Overfit_Gap_Accuracy'] = overfit_gap_acc\n",
    "            \n",
    "            if overfit_gap_f1 > 0.15 or overfit_gap_acc > 0.15:\n",
    "                result['Status'] = 'OVERFITTING'\n",
    "                status_symbol = '‚ö†Ô∏è'\n",
    "            elif result['CV_F1_Mean'] < 0.5 and result['Train_F1_Mean'] < 0.6:\n",
    "                result['Status'] = 'UNDERFITTING'\n",
    "                status_symbol = '‚ö†Ô∏è'\n",
    "            else:\n",
    "                result['Status'] = 'GOOD'\n",
    "                status_symbol = '‚úì'\n",
    "            \n",
    "            cv_results.append(result)\n",
    "            \n",
    "            print(f\" ‚Üí CV F1: {result['CV_F1_Mean']:.4f}, Train F1: {result['Train_F1_Mean']:.4f}, Gap: {result['Overfit_Gap_F1']:.4f} [{result['Status']} {status_symbol}]\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" ‚Üí ERROR: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì CROSS-VALIDATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Results and Identify Best Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cv_results_df_sorted = cv_results_df.sort_values('CV_F1_Mean', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 10 Configurations by CV F1-Score:\")\n",
    "print(cv_results_df_sorted[['Model', 'Strategy', 'CV_F1_Mean', 'CV_ROC_AUC_Mean', \n",
    "                            'Overfit_Gap_F1', 'Status']].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERFITTING/UNDERFITTING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "status_counts = cv_results_df['Status'].value_counts()\n",
    "print(f\"\\nüìà Status Distribution:\")\n",
    "print(status_counts)\n",
    "\n",
    "overfitting = cv_results_df[cv_results_df['Status'] == 'OVERFITTING'].sort_values('Overfit_Gap_F1', ascending=False)\n",
    "if len(overfitting) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è OVERFITTING Detected in {len(overfitting)} configurations:\")\n",
    "    print(overfitting[['Model', 'Strategy', 'Train_F1_Mean', 'CV_F1_Mean', 'Overfit_Gap_F1']].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n‚úì No overfitting detected!\")\n",
    "\n",
    "underfitting = cv_results_df[cv_results_df['Status'] == 'UNDERFITTING']\n",
    "if len(underfitting) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è UNDERFITTING Detected in {len(underfitting)} configurations:\")\n",
    "    print(underfitting[['Model', 'Strategy', 'Train_F1_Mean', 'CV_F1_Mean']].to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n‚úì No underfitting detected!\")\n",
    "\n",
    "good_configs = cv_results_df[cv_results_df['Status'] == 'GOOD'].sort_values('CV_F1_Mean', ascending=False)\n",
    "print(f\"\\n‚úì GOOD Configurations (No Overfitting/Underfitting): {len(good_configs)}\")\n",
    "if len(good_configs) > 0:\n",
    "    print(good_configs[['Model', 'Strategy', 'CV_F1_Mean', 'CV_ROC_AUC_Mean', 'Overfit_Gap_F1']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Cross-Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# Top 15 configurations\n",
    "top_15 = cv_results_df_sorted.head(15)\n",
    "x_labels = [f\"{row['Model'][:15]}\\n{row['Strategy'][:15]}\" for _, row in top_15.iterrows()]\n",
    "x_pos = np.arange(len(top_15))\n",
    "\n",
    "axes[0, 0].bar(x_pos, top_15['CV_F1_Mean'], color='steelblue', alpha=0.7)\n",
    "axes[0, 0].errorbar(x_pos, top_15['CV_F1_Mean'], yerr=top_15['CV_F1_Std'], \n",
    "                    fmt='none', ecolor='red', capsize=3)\n",
    "axes[0, 0].set_title('Top 15 Configurations by CV F1-Score', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_ylabel('F1-Score')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels(x_labels, rotation=45, ha='right', fontsize=8)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# F1-Score by strategy for each model\n",
    "for model in base_models.keys():\n",
    "    model_data = cv_results_df[cv_results_df['Model'] == model]\n",
    "    strategies = model_data['Strategy'].values\n",
    "    f1_scores = model_data['CV_F1_Mean'].values\n",
    "    axes[0, 1].plot(strategies, f1_scores, marker='o', label=model[:15])\n",
    "\n",
    "axes[0, 1].set_title('F1-Score by Resampling Strategy', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_ylabel('CV F1-Score')\n",
    "axes[0, 1].set_xlabel('Resampling Strategy')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].legend(loc='best', fontsize=8)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Overfitting analysis scatter\n",
    "colors = {'GOOD': 'green', 'OVERFITTING': 'red', 'UNDERFITTING': 'orange'}\n",
    "for status in cv_results_df['Status'].unique():\n",
    "    subset = cv_results_df[cv_results_df['Status'] == status]\n",
    "    axes[1, 0].scatter(subset['CV_F1_Mean'], subset['Overfit_Gap_F1'], \n",
    "                      label=status, color=colors.get(status, 'gray'), alpha=0.6, s=80)\n",
    "\n",
    "axes[1, 0].axhline(y=0.15, color='red', linestyle='--', linewidth=1, label='Overfit Threshold')\n",
    "axes[1, 0].set_title('Overfitting Analysis', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_xlabel('CV F1-Score')\n",
    "axes[1, 0].set_ylabel('Overfit Gap (Train F1 - CV F1)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Average performance by strategy\n",
    "strategy_avg = cv_results_df.groupby('Strategy')[['CV_F1_Mean', 'CV_ROC_AUC_Mean']].mean().sort_values('CV_F1_Mean', ascending=False)\n",
    "x_pos_strat = np.arange(len(strategy_avg))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 1].bar(x_pos_strat - width/2, strategy_avg['CV_F1_Mean'], width, label='F1-Score', color='skyblue')\n",
    "axes[1, 1].bar(x_pos_strat + width/2, strategy_avg['CV_ROC_AUC_Mean'], width, label='ROC-AUC', color='coral')\n",
    "axes[1, 1].set_title('Average Performance by Resampling Strategy', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_xticks(x_pos_strat)\n",
    "axes[1, 1].set_xticklabels([s[:15] for s in strategy_avg.index], rotation=45, ha='right')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Select Best Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BEST MODEL SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "good_configs_sorted = good_configs.sort_values('CV_F1_Mean', ascending=False)\n",
    "\n",
    "if len(good_configs_sorted) > 0:\n",
    "    best_config = good_configs_sorted.iloc[0]\n",
    "    print(\"\\n‚úì Selecting from GOOD configurations (no overfitting/underfitting)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No GOOD configurations found. Selecting best overall by F1-Score.\")\n",
    "    best_config = cv_results_df_sorted.iloc[0]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üèÜ BEST MODEL: {best_config['Model']}\")\n",
    "print(f\"üèÜ BEST STRATEGY: {best_config['Strategy']}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"CV F1-Score: {best_config['CV_F1_Mean']:.4f} (+/- {best_config['CV_F1_Std']:.4f})\")\n",
    "print(f\"CV ROC-AUC: {best_config['CV_ROC_AUC_Mean']:.4f}\")\n",
    "print(f\"CV Accuracy: {best_config['CV_Accuracy_Mean']:.4f}\")\n",
    "print(f\"CV Precision: {best_config['CV_Precision_Mean']:.4f}\")\n",
    "print(f\"CV Recall: {best_config['CV_Recall_Mean']:.4f}\")\n",
    "print(f\"Train F1-Score: {best_config['Train_F1_Mean']:.4f}\")\n",
    "print(f\"Overfit Gap (F1): {best_config['Overfit_Gap_F1']:.4f}\")\n",
    "print(f\"Status: {best_config['Status']}\")\n",
    "\n",
    "best_model_class = base_models[best_config['Model']]\n",
    "best_resampler = resampling_strategies[best_config['Strategy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Final Model on Full Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING FINAL MODEL ON FULL TRAINING SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if best_resampler is None:\n",
    "    if hasattr(best_model_class, 'class_weight'):\n",
    "        final_model = best_model_class.__class__(**{**best_model_class.get_params(), 'class_weight': 'balanced'})\n",
    "    else:\n",
    "        final_model = best_model_class.__class__(**best_model_class.get_params())\n",
    "    \n",
    "    X_train_final = X_train\n",
    "    y_train_final = y_train\n",
    "    print(\"\\nUsing original training data (class_weight='balanced')\")\n",
    "else:\n",
    "    final_model = best_model_class.__class__(**best_model_class.get_params())\n",
    "    X_train_final, y_train_final = best_resampler.fit_resample(X_train, y_train)\n",
    "    print(f\"\\nApplied {best_config['Strategy']} resampling\")\n",
    "    print(f\"Original training size: {len(X_train)}\")\n",
    "    print(f\"Resampled training size: {len(X_train_final)}\")\n",
    "    print(f\"Class distribution after resampling:\")\n",
    "    print(pd.Series(y_train_final).value_counts())\n",
    "\n",
    "final_model.fit(X_train_final, y_train_final)\n",
    "print(\"\\n‚úì Final model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Evaluation on Test Set with Overfitting/Underfitting Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL TEST SET EVALUATION\")\n",
    "print(f\"Model: {best_config['Model']} with {best_config['Strategy']}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Training set performance\n",
    "y_train_pred = final_model.predict(X_train_final)\n",
    "y_train_proba = final_model.predict_proba(X_train_final)[:, 1]\n",
    "\n",
    "train_acc = accuracy_score(y_train_final, y_train_pred)\n",
    "train_f1 = f1_score(y_train_final, y_train_pred)\n",
    "train_roc_auc = roc_auc_score(y_train_final, y_train_proba)\n",
    "\n",
    "# Test set performance\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "y_test_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nüìä Training Set Performance:\")\n",
    "print(f\"  Accuracy:  {train_acc:.4f}\")\n",
    "print(f\"  F1-Score:  {train_f1:.4f}\")\n",
    "print(f\"  ROC-AUC:   {train_roc_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä Test Set Performance:\")\n",
    "print(f\"  Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"  F1-Score:  {test_f1:.4f}\")\n",
    "print(f\"  ROC-AUC:   {test_roc_auc:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "\n",
    "final_overfit_gap_f1 = train_f1 - test_f1\n",
    "final_overfit_gap_acc = train_acc - test_acc\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"OVERFITTING/UNDERFITTING FINAL ASSESSMENT\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Overfit Gap (F1): {final_overfit_gap_f1:.4f}\")\n",
    "print(f\"Overfit Gap (Accuracy): {final_overfit_gap_acc:.4f}\")\n",
    "\n",
    "if final_overfit_gap_f1 > 0.15 or final_overfit_gap_acc > 0.15:\n",
    "    final_status = \"‚ö†Ô∏è OVERFITTING DETECTED\"\n",
    "    recommendation = \"Model is overfitting. Consider: regularization, reducing complexity, more data, or different resampling.\"\n",
    "elif test_f1 < 0.5 and train_f1 < 0.6:\n",
    "    final_status = \"‚ö†Ô∏è UNDERFITTING DETECTED\"\n",
    "    recommendation = \"Model is underfitting. Consider: more complex model, feature engineering, or removing regularization.\"\n",
    "else:\n",
    "    final_status = \"‚úì GOOD FIT\"\n",
    "    recommendation = \"Model has good generalization. Performance is balanced between training and test sets.\"\n",
    "\n",
    "print(f\"\\nFinal Status: {final_status}\")\n",
    "print(f\"Recommendation: {recommendation}\")\n",
    "\n",
    "print(f\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['No Claim', 'Claim']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comprehensive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n",
    "            xticklabels=['No Claim', 'Claim'],\n",
    "            yticklabels=['No Claim', 'Claim'])\n",
    "axes[0, 0].set_title('Confusion Matrix - Test Set', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_ylabel('True Label')\n",
    "axes[0, 0].set_xlabel('Predicted Label')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "roc_auc_calc = auc(fpr, tpr)\n",
    "\n",
    "axes[0, 1].plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_calc:.4f})')\n",
    "axes[0, 1].plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Classifier')\n",
    "axes[0, 1].set_xlim([0.0, 1.0])\n",
    "axes[0, 1].set_ylim([0.0, 1.05])\n",
    "axes[0, 1].set_xlabel('False Positive Rate')\n",
    "axes[0, 1].set_ylabel('True Positive Rate')\n",
    "axes[0, 1].set_title('ROC Curve', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].legend(loc=\"lower right\")\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Train vs Test Performance\n",
    "performance_comparison = pd.DataFrame({\n",
    "    'Dataset': ['Training', 'Test'],\n",
    "    'Accuracy': [train_acc, test_acc],\n",
    "    'F1-Score': [train_f1, test_f1],\n",
    "    'ROC-AUC': [train_roc_auc, test_roc_auc]\n",
    "})\n",
    "\n",
    "x_pos = np.arange(len(performance_comparison))\n",
    "width = 0.25\n",
    "\n",
    "axes[1, 0].bar(x_pos - width, performance_comparison['Accuracy'], width, label='Accuracy', color='skyblue')\n",
    "axes[1, 0].bar(x_pos, performance_comparison['F1-Score'], width, label='F1-Score', color='coral')\n",
    "axes[1, 0].bar(x_pos + width, performance_comparison['ROC-AUC'], width, label='ROC-AUC', color='lightgreen')\n",
    "axes[1, 0].set_title('Train vs Test Performance', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels(performance_comparison['Dataset'])\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Feature Importances (if available)\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': final_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False).head(15)\n",
    "    \n",
    "    axes[1, 1].barh(feature_importance['Feature'], feature_importance['Importance'], color='teal')\n",
    "    axes[1, 1].set_title('Top 15 Feature Importances', fontweight='bold', fontsize=12)\n",
    "    axes[1, 1].set_xlabel('Importance')\n",
    "    axes[1, 1].invert_yaxis()\n",
    "elif hasattr(final_model, 'coef_'):\n",
    "    feature_coef = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Coefficient': np.abs(final_model.coef_[0])\n",
    "    }).sort_values('Coefficient', ascending=False).head(15)\n",
    "    \n",
    "    axes[1, 1].barh(feature_coef['Feature'], feature_coef['Coefficient'], color='purple')\n",
    "    axes[1, 1].set_title('Top 15 Feature Coefficients', fontweight='bold', fontsize=12)\n",
    "    axes[1, 1].set_xlabel('Absolute Coefficient')\n",
    "    axes[1, 1].invert_yaxis()\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Feature importance\\nnot available', \n",
    "                   ha='center', va='center', fontsize=12)\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Additional Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Predicted Probability Distribution\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test.values,\n",
    "    'Predicted': y_test_pred,\n",
    "    'Probability_Claim': y_test_proba,\n",
    "    'Correct': y_test.values == y_test_pred\n",
    "})\n",
    "\n",
    "bins = np.linspace(0, 1, 20)\n",
    "axes[0].hist(predictions_df[predictions_df['Actual'] == 0]['Probability_Claim'], \n",
    "            bins=bins, alpha=0.6, label='Actual: No Claim', color='blue')\n",
    "axes[0].hist(predictions_df[predictions_df['Actual'] == 1]['Probability_Claim'], \n",
    "            bins=bins, alpha=0.6, label='Actual: Claim', color='red')\n",
    "axes[0].axvline(x=0.5, color='black', linestyle='--', label='Decision Threshold')\n",
    "axes[0].set_title('Predicted Probability Distribution', fontweight='bold', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted Probability of Claim')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Model Comparison\n",
    "model_comparison = cv_results_df.groupby('Model')[['CV_F1_Mean', 'CV_ROC_AUC_Mean']].mean().sort_values('CV_F1_Mean', ascending=False)\n",
    "x_pos = np.arange(len(model_comparison))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x_pos - width/2, model_comparison['CV_F1_Mean'], width, label='F1-Score', color='steelblue')\n",
    "axes[1].bar(x_pos + width/2, model_comparison['CV_ROC_AUC_Mean'], width, label='ROC-AUC', color='coral')\n",
    "axes[1].set_title('Average Model Performance', fontweight='bold', fontsize=12)\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels([m[:15] for m in model_comparison.index], rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PREDICTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "print(f\"Correct predictions: {predictions_df['Correct'].sum()}\")\n",
    "print(f\"Incorrect predictions: {(~predictions_df['Correct']).sum()}\")\n",
    "print(f\"Accuracy: {predictions_df['Correct'].mean():.4f}\")\n",
    "\n",
    "print(\"\\nüìä Prediction Distribution:\")\n",
    "print(pd.crosstab(predictions_df['Actual'], predictions_df['Predicted'], \n",
    "                  rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "print(\"\\nFirst 20 predictions:\")\n",
    "print(predictions_df.head(20).to_string(index=False))\n",
    "\n",
    "false_positives = predictions_df[(predictions_df['Actual'] == 0) & (predictions_df['Predicted'] == 1)]\n",
    "false_negatives = predictions_df[(predictions_df['Actual'] == 1) & (predictions_df['Predicted'] == 0)]\n",
    "\n",
    "print(f\"\\nFalse Positives: {len(false_positives)}\")\n",
    "print(f\"False Negatives: {len(false_negatives)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£ DATA OVERVIEW\")\n",
    "print(f\"   - Training samples: {len(X_train)}\")\n",
    "print(f\"   - Test samples: {len(X_test)}\")\n",
    "print(f\"   - Features: {X_train.shape[1]}\")\n",
    "print(f\"   - Class imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£ CROSS-VALIDATION APPROACH\")\n",
    "print(f\"   - Method: 5-Fold Stratified Cross-Validation\")\n",
    "print(f\"   - Models tested: {len(base_models)}\")\n",
    "print(f\"   - Resampling strategies tested: {len(resampling_strategies)}\")\n",
    "print(f\"   - Total configurations: {len(cv_results_df)}\")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ BEST MODEL SELECTED\")\n",
    "print(f\"   - Algorithm: {best_config['Model']}\")\n",
    "print(f\"   - Resampling: {best_config['Strategy']}\")\n",
    "print(f\"   - Cross-validation F1: {best_config['CV_F1_Mean']:.4f} (+/- {best_config['CV_F1_Std']:.4f})\")\n",
    "print(f\"   - Cross-validation ROC-AUC: {best_config['CV_ROC_AUC_Mean']:.4f}\")\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£ FINAL TEST PERFORMANCE\")\n",
    "print(f\"   - Accuracy: {test_acc:.4f}\")\n",
    "print(f\"   - F1-Score: {test_f1:.4f}\")\n",
    "print(f\"   - ROC-AUC: {test_roc_auc:.4f}\")\n",
    "print(f\"   - Precision: {test_precision:.4f}\")\n",
    "print(f\"   - Recall: {test_recall:.4f}\")\n",
    "\n",
    "print(f\"\\n5Ô∏è‚É£ OVERFITTING/UNDERFITTING ANALYSIS\")\n",
    "print(f\"   - Train F1-Score: {train_f1:.4f}\")\n",
    "print(f\"   - Test F1-Score: {test_f1:.4f}\")\n",
    "print(f\"   - Overfit Gap (F1): {final_overfit_gap_f1:.4f}\")\n",
    "print(f\"   - Status: {final_status}\")\n",
    "print(f\"   - Recommendation: {recommendation}\")\n",
    "\n",
    "print(f\"\\n6Ô∏è‚É£ RESAMPLING EFFECTIVENESS\")\n",
    "print(f\"   - Configurations with good fit: {len(good_configs)}\")\n",
    "print(f\"   - Configurations with overfitting: {len(overfitting)}\")\n",
    "print(f\"   - Configurations with underfitting: {len(underfitting)}\")\n",
    "\n",
    "if len(good_configs) > 0:\n",
    "    best_strategy_for_good = good_configs['Strategy'].value_counts().index[0]\n",
    "    print(f\"   - Best strategy for avoiding overfitting: {best_strategy_for_good}\")\n",
    "\n",
    "print(f\"\\n7Ô∏è‚É£ KEY INSIGHTS\")\n",
    "print(f\"   ‚úì Cross-validation ensured robust model selection\")\n",
    "print(f\"   ‚úì Tested {len(base_models)} models √ó {len(resampling_strategies)} strategies = {len(cv_results_df)} configurations\")\n",
    "print(f\"   ‚úì Identified and handled class imbalance appropriately\")\n",
    "print(f\"   ‚úì Per-model resampling strategy was optimized\")\n",
    "print(f\"   ‚úì Final model shows {'good generalization' if 'GOOD' in final_status else final_status.lower()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
