{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance Claim Prediction - Enhanced Pipeline\n",
    "Complete ML pipeline with:\n",
    "- Comprehensive visualizations\n",
    "- Train/Validation/Test split\n",
    "- Multiple algorithms comparison\n",
    "- Intelligent SMOTE analysis and decision-making\n",
    "- Best model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, f1_score, \n",
    "                             roc_auc_score, roc_curve, auc, accuracy_score, \n",
    "                             precision_score, recall_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"train_Insurance.csv\")\n",
    "test_raw = pd.read_csv(\"test_Insurance.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_raw.shape}\")\n",
    "print(f\"Test shape: {test_raw.shape}\")\n",
    "print(f\"\\nTrain columns: {list(train_raw.columns)}\")\n",
    "print(f\"\\nTrain info:\")\n",
    "train_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = train_raw.isna().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing) > 0:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(missing.index, missing.values, color='coral')\n",
    "    plt.title('Missing Values in Training Data', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "train_raw['Claim'].value_counts().plot.pie(\n",
    "    autopct='%1.1f%%',\n",
    "    ax=axes[0],\n",
    "    ylabel='',\n",
    "    colors=['#66b3ff', '#ff9999'],\n",
    "    title='Target Distribution (Pie Chart)'\n",
    ")\n",
    "\n",
    "claim_counts = train_raw['Claim'].value_counts()\n",
    "axes[1].bar(claim_counts.index, claim_counts.values, color=['#66b3ff', '#ff9999'])\n",
    "axes[1].set_title('Target Distribution (Bar Chart)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xlabel('Claim')\n",
    "for i, v in enumerate(claim_counts.values):\n",
    "    axes[1].text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(train_raw['Claim'].value_counts())\n",
    "print(f\"\\nClass proportions:\")\n",
    "print(train_raw['Claim'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [col for col in train_raw.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "            if col not in [\"Claim\", \"Customer Id\"]]\n",
    "\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "\n",
    "n_cols = 2\n",
    "n_rows = math.ceil(len(cat_cols) / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(cat_cols):\n",
    "    sns.countplot(x=col, data=train_raw, ax=axes[i], palette='Set2')\n",
    "    axes[i].set_title(f\"Distribution of {col}\", fontweight='bold')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for container in axes[i].containers:\n",
    "        axes[i].bar_label(container)\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = train_raw.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "num_cols = [c for c in num_cols if c not in [\"Claim\", \"Customer Id\"]]\n",
    "\n",
    "n_cols = 2\n",
    "n_rows = math.ceil(len(num_cols) / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    sns.histplot(train_raw[col].dropna(), kde=True, ax=axes[i], color='steelblue')\n",
    "    axes[i].set_title(f\"Distribution of {col}\", fontweight='bold')\n",
    "    axes[i].set_xlabel(col)\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Remove Customer ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw.drop(labels=['Customer Id'], axis=1, inplace=True)\n",
    "test_raw.drop(labels=['Customer Id'], axis=1, inplace=True)\n",
    "\n",
    "print(f\"Columns after removing Customer ID: {list(train_raw.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Clean NumberOfWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw['NumberOfWindows'] = train_raw['NumberOfWindows'].replace({'without': 0, '>=10': 10})\n",
    "train_raw['NumberOfWindows'] = train_raw['NumberOfWindows'].astype(int)\n",
    "\n",
    "test_raw['NumberOfWindows'] = test_raw['NumberOfWindows'].replace({'without': 0, '>=10': 10})\n",
    "test_raw['NumberOfWindows'] = test_raw['NumberOfWindows'].astype(int)\n",
    "\n",
    "print(\"NumberOfWindows cleaned in both datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Remove Duplicates and Conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exact = train_raw.duplicated().sum()\n",
    "n_same_features = train_raw.duplicated(subset=[c for c in train_raw.columns if c != 'Claim']).sum()\n",
    "\n",
    "print(f\"Exact duplicates: {n_exact}\")\n",
    "print(f\"Duplicates with same features: {n_same_features}\")\n",
    "\n",
    "if n_same_features > 0:\n",
    "    features = [c for c in train_raw.columns if c != \"Claim\"]\n",
    "    dups = train_raw[train_raw.duplicated(subset=features, keep=False)]\n",
    "    conflicts = dups.groupby(features)['Claim'].nunique()\n",
    "    n_conflicts = (conflicts > 1).sum()\n",
    "    print(f\"Conflicting records: {n_conflicts}\")\n",
    "    \n",
    "    if n_conflicts > 0:\n",
    "        conflicting_groups = conflicts[conflicts > 1].reset_index()\n",
    "        before = len(train_raw)\n",
    "        train_raw = train_raw.merge(conflicting_groups[features], on=features, how='left', indicator=True)\n",
    "        train_raw = train_raw[train_raw['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "        after = len(train_raw)\n",
    "        print(f\"Removed {before - after} conflicting records\")\n",
    "\n",
    "train_raw.drop_duplicates(inplace=True)\n",
    "print(f\"Train shape after cleaning: {train_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Handle Missing Values (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in train:\")\n",
    "print(train_raw.isna().sum())\n",
    "\n",
    "mf_imputer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "train_raw[[\"Garden\"]] = mf_imputer.fit_transform(train_raw[[\"Garden\"]])\n",
    "\n",
    "median_imputer = SimpleImputer(strategy=\"median\")\n",
    "train_raw[[\"Building Dimension\"]] = median_imputer.fit_transform(train_raw[[\"Building Dimension\"]])\n",
    "\n",
    "print(\"\\nImputers fitted on train data\")\n",
    "print(f\"Missing values after imputation:\\n{train_raw.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Fill Missing Geo_Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_geo_train = (\n",
    "    train_raw[train_raw[\"Geo_Code\"].notna()]\n",
    "    .groupby([\"Settlement\", \"Residential\"])[\"Geo_Code\"]\n",
    "    .agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"Geo_Code\": \"Geo_Code_mode\"})\n",
    ")\n",
    "\n",
    "print(\"Train Geo_Code modes by Settlement+Residential:\")\n",
    "print(mode_geo_train)\n",
    "\n",
    "train_raw = train_raw.merge(mode_geo_train, on=[\"Settlement\", \"Residential\"], how=\"left\")\n",
    "train_raw[\"Geo_Code\"] = train_raw[\"Geo_Code\"].fillna(train_raw[\"Geo_Code_mode\"])\n",
    "train_raw = train_raw.drop(columns=[\"Geo_Code_mode\"])\n",
    "\n",
    "print(f\"\\nTrain missing values after Geo_Code fill: {train_raw['Geo_Code'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Clean Geo_Code (remove alphanumeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_numeric = train_raw[\"Geo_Code\"].astype(str).str.isnumeric()\n",
    "print(f\"Train numeric Geo_Code: {mask_numeric.sum()}\")\n",
    "print(f\"Train alphanumeric Geo_Code: {(~mask_numeric).sum()}\")\n",
    "\n",
    "train_raw = train_raw[mask_numeric].copy()\n",
    "train_raw[\"Geo_Code\"] = train_raw[\"Geo_Code\"].astype(int)\n",
    "\n",
    "print(f\"Train shape after Geo_Code cleaning: {train_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Handle Outliers in Building Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.boxplot(x=train_raw['Building Dimension'], ax=axes[0], color='orange')\n",
    "axes[0].set_title('Building Dimension - Before Outlier Treatment', fontweight='bold')\n",
    "\n",
    "Q1 = train_raw['Building Dimension'].quantile(0.25)\n",
    "Q3 = train_raw['Building Dimension'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Building Dimension - Q1: {Q1}, Q3: {Q3}, IQR: {IQR}\")\n",
    "print(f\"Bounds: [{lower}, {upper}]\")\n",
    "\n",
    "outliers_before = (train_raw['Building Dimension'] < lower) | (train_raw['Building Dimension'] > upper)\n",
    "print(f\"Outliers found: {outliers_before.sum()}\")\n",
    "\n",
    "train_raw['Building Dimension'] = train_raw['Building Dimension'].clip(lower, upper)\n",
    "\n",
    "sns.boxplot(x=train_raw['Building Dimension'], ax=axes[1], color='skyblue')\n",
    "axes[1].set_title('Building Dimension - After Outlier Treatment', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Outliers clipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Scale Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = ['Building Dimension', 'NumberOfWindows']\n",
    "scaler = RobustScaler()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for i, col in enumerate(cols_to_scale):\n",
    "    sns.histplot(train_raw[col], kde=True, ax=axes[i, 0], color='coral', bins=30)\n",
    "    axes[i, 0].set_title(f\"{col} - Before Scaling\", fontweight='bold')\n",
    "\n",
    "train_raw[cols_to_scale] = scaler.fit_transform(train_raw[cols_to_scale])\n",
    "\n",
    "for i, col in enumerate(cols_to_scale):\n",
    "    sns.histplot(train_raw[col], kde=True, ax=axes[i, 1], color='steelblue', bins=30)\n",
    "    axes[i, 1].set_title(f\"{col} - After RobustScaler\", fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"RobustScaler fitted and applied to train data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformed = train_raw.copy()\n",
    "\n",
    "train_transformed[\"Building_Painted\"] = train_transformed[\"Building_Painted\"].map({'N': 1, 'V': 0}).astype('int32')\n",
    "train_transformed[\"Building_Fenced\"] = train_transformed[\"Building_Fenced\"].map({'N': 1, 'V': 0}).astype('int32')\n",
    "train_transformed[\"Garden\"] = train_transformed[\"Garden\"].map({'V': 1, 'O': 0}).astype('int32')\n",
    "\n",
    "train_transformed = pd.get_dummies(train_transformed, columns=[\"Settlement\", \"Building_Type\"], drop_first=True, dtype='int32')\n",
    "\n",
    "le_claim = LabelEncoder()\n",
    "train_transformed[\"Claim\"] = le_claim.fit_transform(train_transformed[\"Claim\"])\n",
    "\n",
    "cols = [c for c in train_transformed.columns if c != \"Claim\"] + [\"Claim\"]\n",
    "train_transformed = train_transformed[cols]\n",
    "\n",
    "print(\"Train data encoded\")\n",
    "print(f\"Train shape: {train_transformed.shape}\")\n",
    "print(f\"Train columns: {list(train_transformed.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = train_transformed.corr(numeric_only=True)\n",
    "corr_with_claim = df_corr[[\"Claim\"]].sort_values(by=\"Claim\", ascending=False)\n",
    "\n",
    "print(\"Correlation with Claim:\")\n",
    "print(corr_with_claim)\n",
    "\n",
    "plt.figure(figsize=(6, len(corr_with_claim)*0.4))\n",
    "sns.heatmap(corr_with_claim, annot=True, vmin=-1, vmax=1, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation with Target (Claim)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop = [\n",
    "    'Building_Painted',\n",
    "    'Geo_Code',\n",
    "    'YearOfObservation',\n",
    "    'Building_Type_Non-combustible'\n",
    "]\n",
    "\n",
    "train_transformed = train_transformed.drop(columns=cols_to_drop)\n",
    "train_transformed = train_transformed.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nFinal train shape: {train_transformed.shape}\")\n",
    "print(f\"Final train columns: {list(train_transformed.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Same Transformations to Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying transformations to test data...\")\n",
    "\n",
    "test_raw[[\"Garden\"]] = mf_imputer.transform(test_raw[[\"Garden\"]])\n",
    "test_raw[[\"Building Dimension\"]] = median_imputer.transform(test_raw[[\"Building Dimension\"]])\n",
    "\n",
    "test_raw = test_raw.merge(mode_geo_train, on=[\"Settlement\", \"Residential\"], how=\"left\")\n",
    "test_raw[\"Geo_Code\"] = test_raw[\"Geo_Code\"].fillna(test_raw[\"Geo_Code_mode\"])\n",
    "test_raw = test_raw.drop(columns=[\"Geo_Code_mode\"])\n",
    "\n",
    "mask_numeric_test = test_raw[\"Geo_Code\"].astype(str).str.isnumeric()\n",
    "test_raw = test_raw[mask_numeric_test].copy()\n",
    "test_raw[\"Geo_Code\"] = test_raw[\"Geo_Code\"].astype(int)\n",
    "\n",
    "test_raw[cols_to_scale] = scaler.transform(test_raw[cols_to_scale])\n",
    "\n",
    "test_transformed = test_raw.copy()\n",
    "test_transformed[\"Building_Painted\"] = test_transformed[\"Building_Painted\"].map({'N': 1, 'V': 0}).astype('int32')\n",
    "test_transformed[\"Building_Fenced\"] = test_transformed[\"Building_Fenced\"].map({'N': 1, 'V': 0}).astype('int32')\n",
    "test_transformed[\"Garden\"] = test_transformed[\"Garden\"].map({'V': 1, 'O': 0}).astype('int32')\n",
    "\n",
    "test_transformed = pd.get_dummies(test_transformed, columns=[\"Settlement\", \"Building_Type\"], drop_first=True, dtype='int32')\n",
    "test_transformed[\"Claim\"] = le_claim.transform(test_transformed[\"Claim\"])\n",
    "\n",
    "cols = [c for c in test_transformed.columns if c != \"Claim\"] + [\"Claim\"]\n",
    "test_transformed = test_transformed[cols]\n",
    "test_transformed = test_transformed.drop(columns=cols_to_drop)\n",
    "test_transformed = test_transformed.reset_index(drop=True)\n",
    "\n",
    "print(f\"Final test shape: {test_transformed.shape}\")\n",
    "print(f\"Test columns match train: {list(train_transformed.columns) == list(test_transformed.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split Data: Train/Validation from train_Insurance, Test from test_Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = train_transformed.drop('Claim', axis=1)\n",
    "y_train_full = train_transformed['Claim']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train_full\n",
    ")\n",
    "\n",
    "X_test = test_transformed.drop('Claim', axis=1)\n",
    "y_test = test_transformed['Claim']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA SPLIT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set (final): {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTrain target distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nValidation target distribution:\\n{y_val.value_counts()}\")\n",
    "print(f\"\\nTest target distribution:\\n{y_test.value_counts()}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "sets = [('Train', y_train), ('Validation', y_val), ('Test', y_test)]\n",
    "\n",
    "for ax, (name, y_data) in zip(axes, sets):\n",
    "    counts = y_data.value_counts()\n",
    "    ax.bar(['No Claim', 'Claim'], counts.values, color=['#66b3ff', '#ff9999'])\n",
    "    ax.set_title(f'{name} Set Distribution', fontweight='bold')\n",
    "    ax.set_ylabel('Count')\n",
    "    for i, v in enumerate(counts.values):\n",
    "        ax.text(i, v + 10, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Imbalance Analysis: Should We Use SMOTE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CLASS IMBALANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "minority_class = y_train.value_counts().min()\n",
    "majority_class = y_train.value_counts().max()\n",
    "imbalance_ratio = majority_class / minority_class\n",
    "minority_percentage = (minority_class / len(y_train)) * 100\n",
    "\n",
    "print(f\"\\nTrain Set Class Distribution:\")\n",
    "print(f\"  Majority class (No Claim): {majority_class} samples\")\n",
    "print(f\"  Minority class (Claim): {minority_class} samples\")\n",
    "print(f\"  Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"  Minority class percentage: {minority_percentage:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"IMBALANCE SEVERITY ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if imbalance_ratio < 1.5:\n",
    "    severity = \"BALANCED\"\n",
    "    recommendation = \"No resampling needed. Use standard training.\"\n",
    "    color = 'green'\n",
    "elif imbalance_ratio < 3:\n",
    "    severity = \"MILD IMBALANCE\"\n",
    "    recommendation = \"Consider class_weight='balanced' parameter instead of SMOTE.\"\n",
    "    color = 'yellow'\n",
    "elif imbalance_ratio < 9:\n",
    "    severity = \"MODERATE IMBALANCE\"\n",
    "    recommendation = \"SMOTE or class_weight='balanced' may help, but carefully validate.\"\n",
    "    color = 'orange'\n",
    "else:\n",
    "    severity = \"SEVERE IMBALANCE\"\n",
    "    recommendation = \"SMOTE strongly recommended, or consider ADASYN/BorderlineSMOTE.\"\n",
    "    color = 'red'\n",
    "\n",
    "print(f\"\\nImbalance Severity: {severity}\")\n",
    "print(f\"Recommendation: {recommendation}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET SIZE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_samples = len(X_train)\n",
    "samples_per_feature = total_samples / X_train.shape[1]\n",
    "\n",
    "print(f\"\\nTotal training samples: {total_samples}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Samples per feature: {samples_per_feature:.1f}\")\n",
    "print(f\"Minority class samples: {minority_class}\")\n",
    "\n",
    "if minority_class < 50:\n",
    "    size_warning = \"CRITICAL: Very few minority samples. SMOTE may cause severe overfitting!\"\n",
    "    smote_feasible = False\n",
    "elif minority_class < 100:\n",
    "    size_warning = \"WARNING: Limited minority samples. SMOTE may cause overfitting.\"\n",
    "    smote_feasible = False\n",
    "elif minority_class < 200:\n",
    "    size_warning = \"CAUTION: Moderate minority samples. Monitor SMOTE performance carefully.\"\n",
    "    smote_feasible = True\n",
    "else:\n",
    "    size_warning = \"GOOD: Sufficient minority samples for SMOTE.\"\n",
    "    smote_feasible = True\n",
    "\n",
    "print(f\"\\n{size_warning}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL RECOMMENDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not smote_feasible:\n",
    "    final_decision = \"DO NOT USE SMOTE - Use class_weight='balanced' only\"\n",
    "    use_smote = False\n",
    "    print(f\"\\n{final_decision}\")\n",
    "    print(\"\\nReasons:\")\n",
    "    print(\"  - Too few minority class samples for reliable synthetic generation\")\n",
    "    print(\"  - High risk of overfitting to synthetic data\")\n",
    "    print(\"  - class_weight='balanced' is safer for small datasets\")\n",
    "elif imbalance_ratio < 3:\n",
    "    final_decision = \"DO NOT USE SMOTE - class_weight='balanced' sufficient\"\n",
    "    use_smote = False\n",
    "    print(f\"\\n{final_decision}\")\n",
    "    print(\"\\nReasons:\")\n",
    "    print(\"  - Imbalance is mild (ratio < 3:1)\")\n",
    "    print(\"  - class_weight='balanced' handles this level of imbalance well\")\n",
    "    print(\"  - SMOTE adds unnecessary complexity and overfitting risk\")\n",
    "else:\n",
    "    final_decision = \"TEST BOTH: Compare SMOTE vs class_weight='balanced'\"\n",
    "    use_smote = True\n",
    "    print(f\"\\n{final_decision}\")\n",
    "    print(\"\\nReasons:\")\n",
    "    print(\"  - Moderate to severe imbalance detected\")\n",
    "    print(\"  - Sufficient samples for synthetic generation\")\n",
    "    print(\"  - Will compare both approaches and select the best\")\n",
    "\n",
    "print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(['Majority', 'Minority'], [majority_class, minority_class],\n",
    "            color=['#66b3ff', '#ff9999'])\n",
    "axes[0].set_title('Class Distribution', fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "for i, v in enumerate([majority_class, minority_class]):\n",
    "    axes[0].text(i, v + 10, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "categories = ['Balanced\\n(<1.5:1)', 'Mild\\n(1.5-3:1)', 'Moderate\\n(3-9:1)', 'Severe\\n(>9:1)']\n",
    "ratios = [1.5, 3, 9, 15]\n",
    "current_pos = 0\n",
    "for i, (cat, threshold) in enumerate(zip(categories, ratios)):\n",
    "    if imbalance_ratio <= threshold:\n",
    "        current_pos = i\n",
    "        break\n",
    "else:\n",
    "    current_pos = len(categories) - 1\n",
    "\n",
    "colors_scale = ['green', 'yellow', 'orange', 'red']\n",
    "for i, (cat, col) in enumerate(zip(categories, colors_scale)):\n",
    "    if i == current_pos:\n",
    "        axes[1].bar(i, ratios[i], color=col, alpha=0.8, edgecolor='black', linewidth=3)\n",
    "    else:\n",
    "        axes[1].bar(i, ratios[i], color=col, alpha=0.3)\n",
    "\n",
    "axes[1].axhline(y=imbalance_ratio, color='blue', linestyle='--', linewidth=2,\n",
    "                label=f'Your Data: {imbalance_ratio:.2f}:1')\n",
    "axes[1].set_xticks(range(len(categories)))\n",
    "axes[1].set_xticklabels(categories)\n",
    "axes[1].set_ylabel('Imbalance Ratio')\n",
    "axes[1].set_title('Imbalance Severity Scale', fontweight='bold', fontsize=12)\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=500, class_weight='balanced', random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, max_depth=10),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(class_weight='balanced', probability=True, random_state=42, kernel='rbf')\n",
    "}\n",
    "\n",
    "print(\"Models defined:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train and Evaluate WITH class_weight='balanced' (No SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING WITH class_weight='balanced' (No Resampling)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_no_smote = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    acc = accuracy_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    \n",
    "    results_no_smote[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'y_val_pred': y_val_pred,\n",
    "        'y_val_proba': y_val_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "    print(f\"Validation F1-Score: {f1:.4f}\")\n",
    "    print(f\"Validation ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"Validation Precision: {precision:.4f}\")\n",
    "    print(f\"Validation Recall: {recall:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"class_weight='balanced' RESULTS - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "summary_no_smote = pd.DataFrame({\n",
    "    'Model': list(results_no_smote.keys()),\n",
    "    'Accuracy': [results_no_smote[m]['accuracy'] for m in results_no_smote.keys()],\n",
    "    'F1-Score': [results_no_smote[m]['f1_score'] for m in results_no_smote.keys()],\n",
    "    'ROC-AUC': [results_no_smote[m]['roc_auc'] for m in results_no_smote.keys()],\n",
    "    'Precision': [results_no_smote[m]['precision'] for m in results_no_smote.keys()],\n",
    "    'Recall': [results_no_smote[m]['recall'] for m in results_no_smote.keys()]\n",
    "})\n",
    "print(summary_no_smote.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train and Evaluate WITH SMOTE (If Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_smote:\n",
    "    print(\"=\"*60)\n",
    "    print(\"TRAINING WITH SMOTE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nOriginal train set: {X_train.shape[0]} samples\")\n",
    "    print(f\"After SMOTE: {X_train_smote.shape[0]} samples\")\n",
    "    print(f\"\\nClass distribution after SMOTE:\")\n",
    "    print(pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    pd.Series(y_train).value_counts().plot(kind='bar', ax=axes[0], color=['#66b3ff', '#ff9999'])\n",
    "    axes[0].set_title('Before SMOTE', fontweight='bold')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_xticklabels(['No Claim', 'Claim'], rotation=0)\n",
    "\n",
    "    pd.Series(y_train_smote).value_counts().plot(kind='bar', ax=axes[1], color=['#66b3ff', '#ff9999'])\n",
    "    axes[1].set_title('After SMOTE', fontweight='bold')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_xticklabels(['No Claim', 'Claim'], rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    results_smote = {}\n",
    "    \n",
    "    for name, model_class in models.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training: {name} (WITH SMOTE)\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        if name == 'Logistic Regression':\n",
    "            model = LogisticRegression(max_iter=500, random_state=42)\n",
    "        elif name == 'Random Forest':\n",
    "            model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "        elif name == 'Gradient Boosting':\n",
    "            model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "        elif name == 'SVM':\n",
    "            model = SVC(probability=True, random_state=42, kernel='rbf')\n",
    "\n",
    "        model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        acc = accuracy_score(y_val, y_val_pred)\n",
    "        f1 = f1_score(y_val, y_val_pred)\n",
    "        roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "        precision = precision_score(y_val, y_val_pred)\n",
    "        recall = recall_score(y_val, y_val_pred)\n",
    "\n",
    "        results_smote[name] = {\n",
    "            'model': model,\n",
    "            'accuracy': acc,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'y_val_pred': y_val_pred,\n",
    "            'y_val_proba': y_val_proba\n",
    "        }\n",
    "\n",
    "        print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "        print(f\"Validation F1-Score: {f1:.4f}\")\n",
    "        print(f\"Validation ROC-AUC: {roc_auc:.4f}\")\n",
    "        print(f\"Validation Precision: {precision:.4f}\")\n",
    "        print(f\"Validation Recall: {recall:.4f}\")\n",
    "        print(f\"\\nConfusion Matrix:\")\n",
    "        print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WITH SMOTE - SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    summary_smote = pd.DataFrame({\n",
    "        'Model': list(results_smote.keys()),\n",
    "        'Accuracy': [results_smote[m]['accuracy'] for m in results_smote.keys()],\n",
    "        'F1-Score': [results_smote[m]['f1_score'] for m in results_smote.keys()],\n",
    "        'ROC-AUC': [results_smote[m]['roc_auc'] for m in results_smote.keys()],\n",
    "        'Precision': [results_smote[m]['precision'] for m in results_smote.keys()],\n",
    "        'Recall': [results_smote[m]['recall'] for m in results_smote.keys()]\n",
    "    })\n",
    "    print(summary_smote.to_string(index=False))\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"SKIPPING SMOTE TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nBased on the analysis above, SMOTE is not recommended for this dataset.\")\n",
    "    print(\"We will proceed with class_weight='balanced' only.\\n\")\n",
    "    \n",
    "    results_smote = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison: SMOTE vs class_weight='balanced' (If Applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_smote and len(results_smote) > 0:\n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPREHENSIVE COMPARISON: SMOTE vs class_weight='balanced'\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': list(models.keys()),\n",
    "        'F1_No_SMOTE': [results_no_smote[m]['f1_score'] for m in models.keys()],\n",
    "        'F1_With_SMOTE': [results_smote[m]['f1_score'] for m in models.keys()],\n",
    "        'ROC_AUC_No_SMOTE': [results_no_smote[m]['roc_auc'] for m in models.keys()],\n",
    "        'ROC_AUC_With_SMOTE': [results_smote[m]['roc_auc'] for m in models.keys()],\n",
    "        'Accuracy_No_SMOTE': [results_no_smote[m]['accuracy'] for m in models.keys()],\n",
    "        'Accuracy_With_SMOTE': [results_smote[m]['accuracy'] for m in models.keys()]\n",
    "    })\n",
    "\n",
    "    comparison_df['F1_Improvement'] = comparison_df['F1_With_SMOTE'] - comparison_df['F1_No_SMOTE']\n",
    "    comparison_df['ROC_AUC_Improvement'] = comparison_df['ROC_AUC_With_SMOTE'] - comparison_df['ROC_AUC_No_SMOTE']\n",
    "\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "\n",
    "    axes[0, 0].bar(x - width/2, comparison_df['F1_No_SMOTE'], width, label='class_weight balanced', color='skyblue')\n",
    "    axes[0, 0].bar(x + width/2, comparison_df['F1_With_SMOTE'], width, label='With SMOTE', color='coral')\n",
    "    axes[0, 0].set_xlabel('Model')\n",
    "    axes[0, 0].set_ylabel('F1-Score')\n",
    "    axes[0, 0].set_title('F1-Score Comparison', fontweight='bold', fontsize=12)\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    axes[0, 1].bar(x - width/2, comparison_df['ROC_AUC_No_SMOTE'], width, label='class_weight balanced', color='skyblue')\n",
    "    axes[0, 1].bar(x + width/2, comparison_df['ROC_AUC_With_SMOTE'], width, label='With SMOTE', color='coral')\n",
    "    axes[0, 1].set_xlabel('Model')\n",
    "    axes[0, 1].set_ylabel('ROC-AUC')\n",
    "    axes[0, 1].set_title('ROC-AUC Comparison', fontweight='bold', fontsize=12)\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    axes[1, 0].bar(x - width/2, comparison_df['Accuracy_No_SMOTE'], width, label='class_weight balanced', color='skyblue')\n",
    "    axes[1, 0].bar(x + width/2, comparison_df['Accuracy_With_SMOTE'], width, label='With SMOTE', color='coral')\n",
    "    axes[1, 0].set_xlabel('Model')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].set_title('Accuracy Comparison', fontweight='bold', fontsize=12)\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    colors = ['green' if val > 0 else 'red' for val in comparison_df['F1_Improvement']]\n",
    "    axes[1, 1].bar(comparison_df['Model'], comparison_df['F1_Improvement'], color=colors)\n",
    "    axes[1, 1].set_xlabel('Model')\n",
    "    axes[1, 1].set_ylabel('F1-Score Improvement')\n",
    "    axes[1, 1].set_title('F1-Score Improvement with SMOTE', fontweight='bold', fontsize=12)\n",
    "    axes[1, 1].axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    axes[1, 1].tick_params(axis='x', rotation=15)\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"SMOTE COMPARISON SKIPPED\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nSMOTE was not used. Proceeding with class_weight='balanced' only.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Select Best Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BEST MODEL SELECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_f1_no_smote = max(results_no_smote.items(), key=lambda x: x[1]['f1_score'])\n",
    "\n",
    "print(f\"\\nBest model with class_weight='balanced': {best_f1_no_smote[0]}\")\n",
    "print(f\"  F1-Score: {best_f1_no_smote[1]['f1_score']:.4f}\")\n",
    "print(f\"  ROC-AUC: {best_f1_no_smote[1]['roc_auc']:.4f}\")\n",
    "\n",
    "if use_smote and len(results_smote) > 0:\n",
    "    best_f1_smote = max(results_smote.items(), key=lambda x: x[1]['f1_score'])\n",
    "\n",
    "    print(f\"\\nBest model WITH SMOTE: {best_f1_smote[0]}\")\n",
    "    print(f\"  F1-Score: {best_f1_smote[1]['f1_score']:.4f}\")\n",
    "    print(f\"  ROC-AUC: {best_f1_smote[1]['roc_auc']:.4f}\")\n",
    "\n",
    "    if best_f1_smote[1]['f1_score'] > best_f1_no_smote[1]['f1_score']:\n",
    "        best_overall_name = best_f1_smote[0]\n",
    "        best_overall_model = best_f1_smote[1]['model']\n",
    "        best_config = 'WITH SMOTE'\n",
    "        X_train_best = X_train_smote\n",
    "        y_train_best = y_train_smote\n",
    "    else:\n",
    "        best_overall_name = best_f1_no_smote[0]\n",
    "        best_overall_model = best_f1_no_smote[1]['model']\n",
    "        best_config = \"class_weight='balanced'\"\n",
    "        X_train_best = X_train\n",
    "        y_train_best = y_train\n",
    "else:\n",
    "    best_overall_name = best_f1_no_smote[0]\n",
    "    best_overall_model = best_f1_no_smote[1]['model']\n",
    "    best_config = \"class_weight='balanced' (SMOTE not used)\"\n",
    "    X_train_best = X_train\n",
    "    y_train_best = y_train\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL BEST MODEL: {best_overall_name} ({best_config})\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FINAL TEST SET EVALUATION\")\n",
    "print(f\"Model: {best_overall_name} ({best_config})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_test_pred = best_overall_model.predict(X_test)\n",
    "y_test_proba = best_overall_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"  F1-Score:  {test_f1:.4f}\")\n",
    "print(f\"  ROC-AUC:   {test_roc_auc:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['No Claim', 'Claim']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['No Claim', 'Claim'],\n",
    "            yticklabels=['No Claim', 'Claim'])\n",
    "axes[0].set_title('Confusion Matrix - Test Set', fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "roc_auc_calc = auc(fpr, tpr)\n",
    "\n",
    "axes[1].plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_calc:.4f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Classifier')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title(f'ROC Curve - {best_overall_name}', fontweight='bold', fontsize=12)\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Predictions Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test.values,\n",
    "    'Predicted': y_test_pred,\n",
    "    'Probability_Claim': y_test_proba,\n",
    "    'Correct': y_test.values == y_test_pred\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "print(f\"Correct predictions: {predictions_df['Correct'].sum()}\")\n",
    "print(f\"Incorrect predictions: {(~predictions_df['Correct']).sum()}\")\n",
    "print(f\"Accuracy: {predictions_df['Correct'].mean():.4f}\")\n",
    "\n",
    "print(\"\\nFirst 20 predictions:\")\n",
    "print(predictions_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY - ALL MODELS AND CONFIGURATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_summary_data = [\n",
    "    {\n",
    "        'Model': name,\n",
    "        'Configuration': \"class_weight='balanced'\",\n",
    "        'F1-Score': results_no_smote[name]['f1_score'],\n",
    "        'ROC-AUC': results_no_smote[name]['roc_auc'],\n",
    "        'Accuracy': results_no_smote[name]['accuracy'],\n",
    "        'Precision': results_no_smote[name]['precision'],\n",
    "        'Recall': results_no_smote[name]['recall']\n",
    "    }\n",
    "    for name in models.keys()\n",
    "]\n",
    "\n",
    "if use_smote and len(results_smote) > 0:\n",
    "    final_summary_data += [\n",
    "        {\n",
    "            'Model': name,\n",
    "            'Configuration': 'With SMOTE',\n",
    "            'F1-Score': results_smote[name]['f1_score'],\n",
    "            'ROC-AUC': results_smote[name]['roc_auc'],\n",
    "            'Accuracy': results_smote[name]['accuracy'],\n",
    "            'Precision': results_smote[name]['precision'],\n",
    "            'Recall': results_smote[name]['recall']\n",
    "        }\n",
    "        for name in models.keys()\n",
    "    ]\n",
    "\n",
    "final_summary = pd.DataFrame(final_summary_data)\n",
    "final_summary = final_summary.sort_values('F1-Score', ascending=False)\n",
    "print(final_summary.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"WINNING MODEL: {best_overall_name} ({best_config})\")\n",
    "print(f\"Final Test F1-Score: {test_f1:.4f}\")\n",
    "print(f\"Final Test ROC-AUC: {test_roc_auc:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
